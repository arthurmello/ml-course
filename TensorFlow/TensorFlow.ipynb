{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version: \"+tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0e3ad",
   "metadata": {},
   "source": [
    "# Les opérations basiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41961bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissez a et b, deux constantes égales à 2 et 3, respectivement\n",
    "a = ###\n",
    "print(\"a = %i\" % a)\n",
    "\n",
    "b = ###\n",
    "print(\"b = %i\" % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcee926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculez c, la somme de a et b\n",
    "c = ###\n",
    "print(\"a + b = %i\" % c)\n",
    "\n",
    "# Calculez d, le produit de a et b\n",
    "d = ###\n",
    "print(\"a * b = %i\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibilité entre Tensors et Numpy Arrays\n",
    "print(\"Opérations entre Tensors et Numpy Arrays \\n\")\n",
    "\n",
    "# Définissez a et b, un tensor et un array, respectivement (dimensions 2 x 2)\n",
    "a = ###\n",
    "print(\"Tensor: \\n a = %s\" % a)\n",
    "\n",
    "b = np.array([[3., 0.],\n",
    "              [5., 1.]], dtype=np.float32)\n",
    "print(\"NumpyArray: \\n b = %s\" % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculez c, la somme de a et b \n",
    "c = ###\n",
    "print(\"a + b = %s\" % c)\n",
    "\n",
    "# Calculez d, le produit de a et b\n",
    "d = ###\n",
    "print(\"a * b = %s\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itérez sur le tensor a (affichez tous ses élements individuelment)\n",
    "print(\"Iterate through Tensor 'a':\")\n",
    "for i in range(###):\n",
    "    for j in range(###):\n",
    "        print(###)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97273c91",
   "metadata": {},
   "source": [
    "# Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset d'entraînement\n",
    "x_train = np.linspace(0, 10, 100)\n",
    "y_train = x_train + np.random.normal(0,1,100)\n",
    "\n",
    "# Nuage de points\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f34ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Nombre de loops\n",
    "training_epochs = 100\n",
    "\n",
    "# Déclarez les poids\n",
    "weight = tf.Variable(0.)\n",
    "bias = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissez l'expression y de la régression linéaire\n",
    "def linreg(x):\n",
    "    y = ###\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissez la \"loss fonction\" (MSE)\n",
    "def squared_error(y_pred, y_true):\n",
    "    return tf.reduce_mean(###)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0806472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînez le modèle\n",
    "for epoch in range(training_epochs):\n",
    "    # Calculez la \"loss function\" dans le contexte Gradient Tape\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_predicted = ###\n",
    "        loss = ###\n",
    "\n",
    "        # Définissez le gradient\n",
    "        gradients = tape.gradient(###)\n",
    "\n",
    "        # Ajustez les poids\n",
    "        weight.assign_sub(###)\n",
    "        bias.assign_sub(###)\n",
    "\n",
    "        # Affichez la sortie\n",
    "        print(f\"Epoch count {epoch}: Loss value: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez les paramètres du modèle (weight et bias)\n",
    "print(weight.numpy())\n",
    "print(bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuage de points + droite de la régression\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(x_train, linreg(x_train), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f61b8",
   "metadata": {},
   "source": [
    "# Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On télécharge les données MNIST\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad640b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les paramètres des données MNIST\n",
    "num_classes = ### # total classes\n",
    "num_features = ### # data features\n",
    "\n",
    "# Les paramètres d'entraînement\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "batch_size = 256\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8503210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "# On applati les images dans un vector 1-D, avec 784 features (28*28).\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "\n",
    "# Normalisation des images: [0, 255] -> [0, 1]\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "\n",
    "# Utilisez tf.data API pour \"shuffle\" et \"batch\" les données\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().###.###.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d927fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du poids avec dimensions [784, 10] (28*28 features des images et le nombre classes)\n",
    "W = tf.###(tf.###([###, ###]), name=\"weight\")\n",
    "\n",
    "# Le biais, format [10] (nombre de classes)\n",
    "b = tf.###(tf.###([###]), name=\"bias\")\n",
    "\n",
    "# Régression logistique (Wx + b).\n",
    "def logistic_regression(x):\n",
    "    # Utilisez softmax pour normalizer les logits en distribution de probas.\n",
    "    return tf.nn.###(tf.###(x, W) + b)\n",
    "\n",
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Encoder les labels via \"one hot encoder\"\n",
    "    y_true = tf.###(###, depth=###)\n",
    "    \n",
    "    # On limite les valeurs pour éviter l'erreur log(0)\n",
    "    y_pred = tf.clip_by_value(###, 1e-9, 1.)\n",
    "    \n",
    "    # On calcule la cross-entropy.\n",
    "    return tf.reduce_###(-tf.reduce_###(### * tf.math.log(###)))\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    # La prédiction est l'index avec le score le plus élevé dans le vector de prédictions (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.###(###, 1), tf.cast(###, tf.int64))\n",
    "    return tf.reduce_###(tf.cast(###, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer : Stochastic gradient descent\n",
    "optimizer = tf.optimizers.###(###)\n",
    "\n",
    "# Processus d'optimisation \n",
    "def run_optimization(x, y):\n",
    "    # Faites les calculs dans un GradientTape pour une différenciation automatique\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = ###(x)\n",
    "        loss = ###(pred, y)\n",
    "    \n",
    "    # Calculez les gradients\n",
    "    gradients = g.gradient(###, [###, ###])\n",
    "    \n",
    "    # Mettez W et b à jour, en utilisant les gradients\n",
    "    optimizer.apply_gradients(zip(###, [###, ###]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54307846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancez le training pour un nombre donné de steps\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Lancez l'optimisation pour mettre à jour W et b\n",
    "    run_optimization(###, ###)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = logistic_regression(###)\n",
    "        loss = cross_entropy(###, ###)\n",
    "        acc = accuracy(###, ###)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa854e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testez le modèle sur le test set\n",
    "pred = (###(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66593c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez le modèle sur 5 images du test set\n",
    "n_images = 5\n",
    "test_images = x_test[:n_images]\n",
    "predictions = (###(test_images)\n",
    "\n",
    "# Affichez l'image et la prédiction\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203da96",
   "metadata": {},
   "source": [
    "# Réseau de neuronnes v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387d5df",
   "metadata": {},
   "source": [
    "D'abord, nous créons des données aléatoires. x est 1-D tensor, et le modèle doit prédire une valeur y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d259fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.###([[1.,2.]])\n",
    "y = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed6849",
   "metadata": {},
   "source": [
    "Les paramètres sont initisialisés avec une distribution normale, de moyenne 0 et variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fcd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_parameters(size, variance=1.0):\n",
    "    return tf..###((tf.random.normal(size) * variance))\n",
    "\n",
    "first_layer_output_size = 3\n",
    "\n",
    "weights_1 = initalize_parameters((x.shape[1], first_layer_output_size))\n",
    "bias_1 = initalize_parameters([1])\n",
    "\n",
    "\n",
    "weights_2 = initalize_parameters((first_layer_output_size,1))\n",
    "bias_2 = initalize_parameters([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1042c3c",
   "metadata": {},
   "source": [
    "Le réseau de neuronnes doit contenir 2 fonctions linéaires et une fonction non-linéaire entre elles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a4446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neural_network(xb):\n",
    "    # linéaire (1,2 @ 2,3 = 1,3)\n",
    "    l1 = tf.###(###, ###) + ###\n",
    "    \n",
    "    # non-linéaire\n",
    "    l2 = tf.math.maximum(###, tf.Variable([0.]))\n",
    "    \n",
    "    # linéaire (1,3 @ 3,1 = 1,1)\n",
    "    l3 = tf.###(###, ###) + ###\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4be8d",
   "metadata": {},
   "source": [
    "La \"loss function\" (fonction de perte) mésure la distance entre les prédictions et les valeurs réelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(preds, yb):\n",
    "    # Mean Squared Error (MSE)\n",
    "    return tf.math..###((preds-yb)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf2970",
   "metadata": {},
   "source": [
    "\"Learning rate\" (taux d'apprentissage) diminue le gradient, pour que les paramètres ne changent pas trop entre chaque step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b85cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.###([10E-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b587bd",
   "metadata": {},
   "source": [
    "L'entraînement contient 3 étapes:\n",
    "\n",
    "1. Faire la prédiction\n",
    "2. Calculer la distance entre la prédiction et la vraie valeur (quand on calcule cela, le gradient est automatiquement calculé, donc on n'a pas besoin d'y penser)\n",
    "3. Mettre à jour les paramètres en faisant la soustraction du gradient * learning rate\n",
    "\n",
    "Le programme tourne jusqu'au moment où la perte est inférieure ou égale à 0.1.<br>\n",
    "A la fin, on plot la variaation de la perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "while(len(losses) == 0 or losses[-1] > 0.1):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # 1. Prédiction\n",
    "        preds = ###(###)\n",
    "    # 2. Distance entre prédiction et vraie valeur (perte)\n",
    "        loss = ###(###, ###)\n",
    "    dW1, db1, dW2, db2 = tape.gradient(###, \n",
    "                                       [###, ###, \n",
    "                                        ###, ###])\n",
    "\n",
    "    # 3. MaJ des paramètres\n",
    "    weights_1.assign_sub(dW1 * lr)\n",
    "    bias_1.assign_sub(### * lr)\n",
    "\n",
    "    weights_2.assign_sub(### * ###)\n",
    "    bias_2.assign_sub(### * ###)\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.plot(list(range(len(losses))), losses)\n",
    "plt.ylabel('loss (MSE)')\n",
    "plt.xlabel('steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894d9b6",
   "metadata": {},
   "source": [
    "# Réseau de neuronnes v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53830ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les paramètres d'entraînement\n",
    "learning_rate = 0.001\n",
    "training_steps = 3000\n",
    "batch_size = 256\n",
    "display_step = 100\n",
    "\n",
    "# Paramètres du réseau\n",
    "n_hidden_1 = 128 # 1ère couche: nombre de neurones.\n",
    "n_hidden_2 = 256 # 2ème couche: nombre de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des poids et des biais\n",
    "random_normal = tf.initializers.###\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(random_normal([###, ###])),\n",
    "    'h2': tf.Variable(random_normal([###, ###])),\n",
    "    'out': tf.Variable(random_normal([###, ###]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([###])),\n",
    "    'b2': tf.Variable(tf.zeros([###])),\n",
    "    'out': tf.Variable(tf.zeros([###]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92215d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez le modèle\n",
    "def neural_net(x):\n",
    "    # Couche cachée avec 128 neurones\n",
    "    layer_1 = tf.add(tf.###(###, ###), ###)\n",
    "    \n",
    "    # Appliquez la fonction sigmoid à l'output de layer_1\n",
    "    layer_1 = tf.nn.###(###)\n",
    "    \n",
    "    # Couche cachée avec 256 neurones\n",
    "    layer_2 = tf.add(tf.###(###, ###), ###)\n",
    "                     \n",
    "    # Appliquez la fonction sigmoid à l'output de layer_2\n",
    "    layer_2 = tf.nn.###(###)\n",
    "    \n",
    "    # Output: couche avec un neurone par classe\n",
    "    out_layer = tf.matmul(###, ###) + ###\n",
    "    \n",
    "    # Utilisez softmax pour normalizer les logits en distribution de probas.\n",
    "    return tf.nn.###(###)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer : Stochastic gradient descent\n",
    "optimizer = tf.optimizers.###(learning_rate)\n",
    "\n",
    "# Processus d'optimisation \n",
    "def run_optimization(x, y):\n",
    "    # Faites les calculs dans un GradientTape pour une différenciation automatique\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = ###(###)\n",
    "        loss = ###(###, ###)\n",
    "        \n",
    "    # Variables à mettre à jour\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    # Calcul des gradients\n",
    "    gradients = g.###(###, ###)\n",
    "    \n",
    "    # Mettez W et b à jour, en utilisant les gradients\n",
    "    optimizer.###(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0418524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancez le training pour un nombre donné de steps\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Lancez l'optimisation pour mettre à jour W et b\n",
    "    run_optimization(###, ###)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47150de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testez le modèle sur le test set\n",
    "pred = ###(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d632421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez le modèle sur 5 images du test set\n",
    "n_images = 5\n",
    "test_images = x_test[:n_images]\n",
    "predictions = neural_net(###)\n",
    "\n",
    "# Affichez l'image et la prédiction\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef5af7",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les paramètres d'entraînement\n",
    "learning_rate = 0.001\n",
    "training_steps = 200\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Paramètres du réseau\n",
    "conv1_filters = 32 # nombre de filtres pour la 1ère couche convolutionnelle\n",
    "conv2_filters = 64 # nombre de filtres pour la 2ème couche convolutionnelle\n",
    "fc1_units = 1024 # nombre de filtres pour la 1ère couche \"fully connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02674e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisez tf.data API pour \"shuffle\" et \"batch\" les données\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().###(5000).###(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée des wrappers pour la simplicité \n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Création d'un wrapper Conv2D avec un biais et une activation \"relu\"\n",
    "    x = tf.nn.###(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.###(x, b)\n",
    "    return tf.nn.###(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # wrapper MaxPool2D\n",
    "    return tf.nn.###(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des poids et des biais\n",
    "random_normal = tf.initializers.###\n",
    "\n",
    "weights = {\n",
    "    # Couche conv. 1: 5x5 conv, 1 input, 32 filtres (MNIST n'a qu'un seul color channel).\n",
    "    'wc1': tf.Variable(random_normal([###, ###, ###, ###])),\n",
    "    # Couche conv 2: 5x5 conv, 32 inputs, 64 filtres.\n",
    "    'wc2': tf.Variable(random_normal([###, ###, ###, ###])),\n",
    "    # Couche \"fully connected\" 1: 7*7*64 inputs, 1024 units.\n",
    "    'wd1': tf.Variable(random_normal([###*###*###, ###])),\n",
    "    # Couche \"fully connected\" externe: 1024 inputs, 10 units (nombre total de classes)\n",
    "    'out': tf.Variable(random_normal([###, ###]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.zeros([###])),\n",
    "    'bc2': tf.Variable(tf.zeros([###])),\n",
    "    'bd1': tf.Variable(tf.zeros([###])),\n",
    "    'out': tf.Variable(tf.zeros([###]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a73a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "def conv_net(x):\n",
    "    \n",
    "    # Format de l'input : [-1, 28, 28, 1]. Un batch d'images 28x28x1 (grayscale).\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # Couche conv. Format de sortie : [-1, 28, 28, 32].\n",
    "    conv1 = ###(x, weights['wc1'], biases['bc1'])\n",
    "    \n",
    "    # Max Pooling (down-sampling). Format de sortie : [-1, 14, 14, 32].\n",
    "    conv1 = ###(###, k=2)\n",
    "\n",
    "    # Couche conv. Format de sortie : [-1, 14, 14, 64].\n",
    "    conv2 = ###(###, weights['wc2'], biases['bc2'])\n",
    "    \n",
    "    # Max Pooling (down-sampling). Format de sortie : [-1, 7, 7, 64].\n",
    "    conv2 = ###(###, k=2)\n",
    "\n",
    "    # Reshape conv2 output to fit fully connected layer input, Format de sortie : [-1, 7*7*64].\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    \n",
    "    # Couche \"fully connected\". Format de sortie : [-1, 1024].\n",
    "    fc1 = tf.add(tf.matmul(###, weights[###]), biases[###])\n",
    "    \n",
    "    # Apply ReLU to fc1 output for non-linearity.\n",
    "    fc1 = tf.nn.###(###)\n",
    "\n",
    "    # Couche \"fully connected\". Format de sortie : [-1, 10].\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    \n",
    "    # Utilisez softmax pour normalizer les logits en distribution de probas.\n",
    "    return tf.nn.###(###)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer : ADAM\n",
    "optimizer = tf.optimizers.###(###)\n",
    "\n",
    "# Processus d'optmisation\n",
    "def run_optimization(x, y):\n",
    "    # Faites les calculs dans un GradientTape pour une différenciation automatique\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = ###(x)\n",
    "        loss = ###(pred, y)\n",
    "        \n",
    "    # Variables à mettre à jour\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    # Calcul des gradients\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Mettez W et b à jour, en utilisant les gradients\n",
    "    optimizer.###(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancez le training pour un nombre donné de steps\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Lancez l'optimisation pour mettre à jour W et b\n",
    "    run_optimization(###, ###)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = ###(batch_x)\n",
    "        loss = ###(pred, batch_y)\n",
    "        acc = ###(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1332aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testez le modèle sur le test set\n",
    "pred = conv_net(x_test)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43959fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez le modèle sur 5 images du test set\n",
    "n_images = 5\n",
    "test_images = x_test[:n_images]\n",
    "predictions = conv_net(test_images)\n",
    "\n",
    "# Display image and model prediction.\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca928a8c",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533627c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin pour enregistrer les logs\n",
    "logs_path = '/tmp/tensorflow_logs/example/'\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "learning_rate = 0.001\n",
    "training_steps = 3000\n",
    "batch_size = 256\n",
    "display_step = 100\n",
    "\n",
    "# Paramètres du réseau de neurones\n",
    "n_hidden_1 = 128 # 1st layer number of neurons.\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848eaa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisez tf.data API pour \"shuffle\" et \"batch\" les données\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des poids et des poids\n",
    "random_normal = tf.initializers.RandomNormal()\n",
    "\n",
    "weights = {\n",
    "    'h1_weights': tf.Variable(random_normal([num_features, n_hidden_1]), name='h1_weights'),\n",
    "    'h2_weights': tf.Variable(random_normal([n_hidden_1, n_hidden_2]), name='h2_weights'),\n",
    "    'logits_weights': tf.Variable(random_normal([n_hidden_2, num_classes]), name='logits_weights')\n",
    "}\n",
    "biases = {\n",
    "    'h1_bias': tf.Variable(tf.zeros([n_hidden_1]), name='h1_bias'),\n",
    "    'h2_bias': tf.Variable(tf.zeros([n_hidden_2]), name='h2_bias'),\n",
    "    'logits_bias': tf.Variable(tf.zeros([num_classes]), name='logits_bias')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The computation graph to be traced.\n",
    "@tf.function\n",
    "def neural_net(x):\n",
    "    with tf.name_scope('Model'):\n",
    "        with tf.name_scope('HiddenLayer1'):\n",
    "        # Hidden fully connected layer with 128 neurons.\n",
    "            layer_1 = tf.add(tf.matmul(x, weights['h1_weights']), biases['h1_bias'])\n",
    "            # Apply sigmoid to layer_1 output for non-linearity.\n",
    "            layer_1 = tf.nn.sigmoid(layer_1)\n",
    "        with tf.name_scope('HiddenLayer2'):\n",
    "            # Hidden fully connected layer with 256 neurons.\n",
    "            layer_2 = tf.add(tf.matmul(layer_1, weights['h2_weights']), biases['h2_bias'])\n",
    "            # Apply sigmoid to layer_2 output for non-linearity.\n",
    "            layer_2 = tf.nn.sigmoid(layer_2)\n",
    "        with tf.name_scope('LogitsLayer'):\n",
    "            # Output fully connected layer with a neuron for each class.\n",
    "            out_layer = tf.matmul(layer_2, weights['logits_weights']) + biases['logits_bias']\n",
    "            # Apply softmax to normalize the logits to a probability distribution.\n",
    "            out_layer = tf.nn.softmax(out_layer)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy loss function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    with tf.name_scope('CrossEntropyLoss'):\n",
    "        # Encode label to a one hot vector.\n",
    "        y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "        # Clip prediction values to avoid log(0) error.\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "        # Compute cross-entropy.\n",
    "        return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(y_pred, y_true):\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # La prédiction est l'index avec le score le plus élevé dans le vector de prédictions (i.e. argmax).\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "    \n",
    "# Optimizer : Stochastic gradient descent\n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processus d'optimisation \n",
    "def run_optimization(x, y):\n",
    "    # Faites les calculs dans un GradientTape pour une différenciation automatique\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = neural_net(x)\n",
    "        loss = cross_entropy(pred, y)\n",
    "        \n",
    "    # Variables à mettre à jour\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "\n",
    "    # Calcul des gradients\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Mettez W et b à jour, en utilisant les gradients\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des poids et biais en forme d'histogramme dans Tensorboard\n",
    "def summarize_weights(step):\n",
    "    for w in weights:\n",
    "        tf.summary.histogram(w.replace('_', '/'), weights[w], step=step)\n",
    "    for b in biases:\n",
    "        tf.summary.histogram(b.replace('_', '/'), biases[b], step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b70e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un \"Summary Writer\" pour enregistrer les metriques dans Tensorboard\n",
    "summary_writer = tf.summary.create_file_writer(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancez le training pour un nombre donné de steps\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    \n",
    "    # On \"trace\" le \"computation graph\". IL ne change pas, donc on ne l'exporte que la 1ère fois\n",
    "    if step == 1:\n",
    "        tf.summary.trace_on(graph=True, profiler=True)\n",
    "    \n",
    "    # On lance l'optimisation (computation graph).\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    # Exportation du computation graph\n",
    "    if step == 1:\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.trace_export(\n",
    "                  name=\"trace\",\n",
    "                  step=0,\n",
    "                  profiler_outdir=logs_path)\n",
    "\n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "        \n",
    "        # Enregistrement des métriques de loss/acc et des poids dans Tensorboard\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step=step)\n",
    "            tf.summary.scalar('accuracy', acc, step=step)\n",
    "            summarize_weights(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d66b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=/tmp/tensorflow_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f3ee5",
   "metadata": {},
   "source": [
    "Dans un nouvel onglet, allez sur http://localhost:6006/#scalars (n'attendez pas que la commande d'au-dessous soit finie)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
